{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPLhBFZ6RV523BxWK7Kj9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albim72/ML_ZAAWANSOWANY_11/blob/main/DCGAN_mnist_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "02NdpTcg1gF9"
      },
      "outputs": [],
      "source": [
        "from  __future__ import absolute_import\n",
        "from  __future__ import division\n",
        "from  __future__ import print_function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Activation,Dense,Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape,Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "LSvRQQPw5enl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import argparse"
      ],
      "metadata": {
        "id": "KACijbyQ6WRs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(inputs,labels,image_size):\n",
        "  \"\"\"\n",
        "  konstruowanie modelu generatora\n",
        "  wejścia są łączone przed warstwą gęstą\n",
        "  Stos warstw BN-Relu-Cov2DTranspose do generowania fałszywych obrazów\n",
        "  funkcja aktywacji - sigmoid\n",
        "\n",
        "  Argumenty:\n",
        "  inputs  - warstwa wejściowa genezarora - wektor z\n",
        "  labels - warstwa wejściowa dla wektora OH - nałożenie warunków wejścia\n",
        "  image_size - docelowy rozmiar jednego bloku (kwadrat)\n",
        "\n",
        "  Wyjście -> model generatora\n",
        "  \"\"\"\n",
        "\n",
        "  image_resize = image_size//4\n",
        "  kernel_size = 5\n",
        "  layer_filters = [128,64,32,1]\n",
        "\n",
        "  x = concatenate([inputs,labels],axis=1)\n",
        "  x = Dense(image_resize*image_resize*layer_filters[0])(x)\n",
        "  x = Reshape((image_resize,image_size,layer_filters[0]))(x)\n",
        "  for filters in layer_filters:\n",
        "    if filters>layer_filters[-2]:\n",
        "      strides=2\n",
        "    else:\n",
        "      strides=1\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides = strides,\n",
        "                        padding='same')(x)\n",
        "  x = Activation('sigmoid')(x)\n",
        "  generator = Model([inputs,labels],x,name='generator')\n",
        "  return generator"
      ],
      "metadata": {
        "id": "Ux_iQ0wQ6sIC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(inputs,labels,image_size):\n",
        "  \"\"\"\n",
        "  konstruowanie modelu dyskryminatora\n",
        "  wejścia są łączone w warstwie gęstej\n",
        "  Stos warstw LeakyRelu - Conv2D do odróżniania prawdziwych i fałszywych obrazów\n",
        "\n",
        "  Argumenty:\n",
        "  inputs  - warstwa wejściowa dyskryminatora - wektor z\n",
        "  labels - warstwa wejściowa dla wektora OH - nałożenie warunków wejścia\n",
        "  image_size - docelowy rozmiar jednego bloku (kwadrat)\n",
        "\n",
        "  Wyjście -> model dyskryminatora\n",
        "  \"\"\"\n",
        "  kernel_size = 5\n",
        "  layer_filters = [32,64,128,256]\n",
        "\n",
        "  x = inputs\n",
        "  y = Dense(image_size*image_size)(labels)\n",
        "  y = Reshape((image_size,image_size,1))(y)\n",
        "  x = concatenate([x,y])\n",
        "\n",
        "  for filters in layer_filters:\n",
        "    if filters == layer_filters[-1]:\n",
        "      strides = 1\n",
        "    else:\n",
        "      strides = 2\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=strides,\n",
        "               padding='same')(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1)(x)\n",
        "  x = Activation('sigmoid')(x)\n",
        "  discriminator = Model([inputs,labels],x,name='discriminator')\n",
        "  return discriminator"
      ],
      "metadata": {
        "id": "tkWkSBThIbp8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trenowanie dyskryminatora i sieci współzawodniczącej (generatora)"
      ],
      "metadata": {
        "id": "y6-08f_FLAnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(generator,\n",
        "                noise_input,\n",
        "                noise_class,\n",
        "                show=False,\n",
        "                step=0,\n",
        "                model_name = \"gan\"):\n",
        "  os.makedirs(model_name,exist_ok=True)\n",
        "  filename = os.path.join(model_name,\"%05d.png\" % step)\n",
        "  images = generator.predict([noise_input,noise_class])\n",
        "  print(model_name,\" labels -> generated images: \", np.argmax(noise_class,axis=1))\n",
        "  plt.figure(figsize=(2.2,2.2))\n",
        "  num_images = images.shape[0]\n",
        "  image_size = images.shape[1]\n",
        "  rows = int(math.sqrt(noise_input.shape[0]))\n",
        "  for i in range(num_images):\n",
        "    plt.subplot(rows,rows,i+1)\n",
        "    image = np.reshape(images[i], [image_size,image_size])\n",
        "    plt.imshow(image,cmap='gray')\n",
        "    plt.axis('off')\n",
        "  plt.savefig(filename)\n",
        "  if show:\n",
        "    plt.show()\n",
        "  else:\n",
        "    plt.close('all')"
      ],
      "metadata": {
        "id": "mT5-inTSR-Iw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(models,data,params):\n",
        "  \"\"\"\n",
        "  naprzemienne trenowanie dyskryminatora i sieci współzawodniczącej (generatora) przez próbki danych.\n",
        "  1. Trening dyskryminatora -> analiza próbki z poprawnie zaetykietowanymi obrazami i sztucznymi.\n",
        "  2. Trenowanie generatora -> sztuczne obrazy\n",
        "  3. Wyjścia dyskryminatora są warunkowane przez etykiety ze zbioru treningowego dla obrazów prawdziwych i etykietami losowymi dla fałszywych\n",
        "  argumenty:\n",
        "\n",
        "  models - generator,dyskryminator, model sieci współzawodniczącej\n",
        "  data - x_train, y_train\n",
        "  params: parametry sieci\n",
        "  \"\"\"\n",
        "\n",
        "  generator,discriminator,adversarial = models\n",
        "  x_train, y_train = data\n",
        "  batch_size, latent_size, train_steps, num_labels, model_name = params\n",
        "\n",
        "  save_interval = 500\n",
        "  #wektor szumu do oceny postępów generatora\n",
        "  noise_input = np.random.uniform(-1.0,1.0,size=[16,latent_size])\n",
        "  #etykieta dla warunkowania szumu\n",
        "  noise_class = np.eye(num_labels)[np.arange(0,16) % num_labels]\n",
        "  train_size = x_train.shape[0]\n",
        "\n",
        "  print(model_name,\"Etykiety dla generowanych obrazów\", np.argmax(noise_class,axis=1))\n",
        "\n",
        "  for i in range(train_steps):\n",
        "    rand_indexes = np.random.randint(0,train_size,size=batch_size)\n",
        "    real_images = x_train[rand_indexes]\n",
        "    real_labels = y_train[rand_indexes]\n",
        "    noise = np.random.uniform(-1.0,1.0,size=[batch_size,latent_size])\n",
        "    #losowe etykiety\n",
        "    fake_labels = np.eye(num_labels)[np.random.choice(num_labels,batch_size)]\n",
        "\n",
        "    #generowanie sztucznych obrazów opisywanych sztucznymi etykietami\n",
        "    fake_images = generator.predict([noise,fake_labels])\n",
        "    #partia danych uczących = 1 prawdziwy + 1 wygenerowany\n",
        "    x = np.concatenate((real_images,fake_images))\n",
        "    labels = np.concatenate((real_labels,fake_labels))\n",
        "\n",
        "    y = np.ones([2*batch_size,1])\n",
        "    y[batch_size:,:] = 0.0\n",
        "\n",
        "    #uczenie sieci dyskryminatora\n",
        "    loss,acc = discriminator.train_on_batch([x,labels],y)\n",
        "    log = f\"{i}: [dicriminator loss: {loss}, accuracy: {acc}]\"\n",
        "\n",
        "    #uczenie sieci współzawodniczącej\n",
        "    noise = np.random.uniform(-1.0,1.0,size=[batch_size,latent_size])\n",
        "    fake_labels = np.eye(num_labels)[np.random.choice(num_labels,batch_size)]\n",
        "\n",
        "    y = np.ones([batch_size,1])\n",
        "\n",
        "    loss,acc = adversarial.train_on_batch([noise,fake_labels],y)\n",
        "    log = f\"{i}: [adversarial loss: {loss}, accuracy: {acc}]\"\n",
        "    print(log)\n",
        "\n",
        "    if(i+1) % save_interval == 0:\n",
        "      plot_images(generator,\n",
        "                  noise_input = noise_input,\n",
        "                  noise_class = noise_class,\n",
        "                  show=False,\n",
        "                  step=(i+1),\n",
        "                  model_name = model_name)\n",
        "  generator.save(model_name + \".h5\")\n"
      ],
      "metadata": {
        "id": "qrthOprPLG-t"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}